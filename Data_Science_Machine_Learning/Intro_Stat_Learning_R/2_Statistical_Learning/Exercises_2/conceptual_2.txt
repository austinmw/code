An Introduction to Statistical Learning Unofficial Solutions:
http://blog.princehonest.com/stat-learning/

My solutions:

1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors p is small.
A: with a large number of observations, a flexible learning method would be able to learn the signal without as much fear of overfitting

(b) The number of predictors p is extremely large, and the number of observations n is small.
A: If the number of predictors p is very large and n small then there is a greater possibility that a flexible learning method would overfit. Then we expect the inflexible method to be better in this case.

(c) The relationship betweeen the predictors and response is highly non-linear.
A: Would require a flexible statistical learning method to perform optimally

(d) The variance of the error terms, i.e. std^2 = Var(e), is extremely high.
A: With a very large error term variance there is more worry about overfitting and thus an inflexible method would perform better

-----

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. WE are interested in understanding which factors affect CEO salary. 
A: Regression problem, interested in inference, n = 500, p = 4

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.
A: classification, prediction, n = 20, p = 14

(c) We are interested in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.
A: regression, prediction, n = 52, p = 4

-----

3. We now revisit the bias-variance decomposition

(a) Provide a sketch of a typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.
A: see jpg in solutions folder

(b) Explain why each of the five curves has the shape displayed in part (a).
A: 
squared bias: decreases monotonically because increases in flexibility yield a closer fit
variance: increases monotonically because increases in flexibility yield overfit
training error: decreases monotonically because increases in flexbility yield a closer fit
test error: concave up curve because increase in flexibility yields a closer fit before it overfits
Bayes (irreducible error): defines the lower limit, the test error is bounded below by the irreducible error due to variance in the error (epsilon) in the output values (value >= 0). When the training error is lower than the irreducible error, overfitting has taken place.
The Bayes error rate is defined for classification problems and is determined by 
the ratio of data points which lie at the 'wrong' side of the decision boundary, 
(0 <= value < 1).

-----

4. You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction?
A: 
i. stock market price direction, prediction, response: up, down,
input: yesterday's price movement % change, two previous day price movement %
change, etc.

ii. illness classification, inference, response: ill, healthy, input: resting
heart rate, resting breath rate, mile run time

iii. car part replacement, prediction, response: needs to be replace, good,
input: age of part, mileage used for, current amperage

(b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction?
A:
i. CEO salary. inference. predictors: age, industry experience, industry,
years of education. response: salary.

ii. car part replacement. inference. response: life of car part. predictors: age
of part, mileage used for, current amperage.

iii. illness classification, prediction, response: age of death,
input: current age, gender, resting heart rate, resting breath rate, mile run
time.

(c) Describe three real-life applications in which cluster analysis might be useful.
A: 
i. cancer type clustering. diagnose cancer types more accurately.

ii. Netflix movie recommendations. recommend movies based on users who have
watched and rated similar movies.

iii. marketing survey. clustering of demographics for a product(s) to see which
clusters of consumers buy which products.

-----

5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preffered to a less flexible approach? When might a less flexible approach be preferred?
A: 
The advantages for a very flexible approach for regression or classification
are obtaining a better fit for non-linear models, decreasing bias.

The disadvantages for a very flexible approach for regression or classification
are requires estimating a greater number of parameters, follow the noise too
closely (overfit), increasing variance.

A more flexible approach would be preferred to a less flexible approach when we
are interested in prediction and not the interpretability of the results.

A less flexible approach would be preferred to a more flexible approach when we
are interested in inference and the interpretability of the results.

-----

6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as aopposed to a non-parametric approach)? What are its disadvantages?
A:
A parametric approach reduces the problem of estimating f down to one of
estimating a set of parameters because it assumes a form for f.

A non-parametric approach does not assume a functional form for f and so
requires a very large number of observations to accurately estimate f.

The advantages of a parametric approach to regression or classification are the
simplifying of modeling f to a few parameters and not as many observations are
required compared to a non-parametric approach.

The disadvantages of a parametric approach to regression or classification
are a potential to inaccurately estimate f if the form of f assumed is wrong or
to overfit the observations if more flexible models are used.

-----

7. The table below (book pg.53) provides a training data set containing six observations, three predictors, and one qualitative response variable. Supposed we wish to use this data set to make a prediction for Y when X_1 = X_2 = X_3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and the test point, X_1 = X_2 = X_3 = 0.
A: 
   Obs.   X1   X2   X3  Distance(0, 0, 0)   Y
   ---------------------------------------------
   1      0    3    0   3                   Red 
   2      2    0    0   2                   Red
   3      0    1    3   sqrt(10) ~ 3.2      Red
   4      0    1    2   sqrt(5) ~ 2.2       Green
   5      -1   0    1   sqrt(2) ~ 1.4       Green
   6      1    1    1   sqrt(3) ~ 1.7       Red

(b) What is our prediction with K = 1? Why?
A: When K = 1, prediction will be Green because 1.4 is the shortest distance

(c) What is our prediction with K = 3? Why?
A: When K = 3, prediction will be Red because 2/3 Red 1/3 Green

(d) If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best fit for K to be large or small? Why?
A: If the decision boundary is highly non-linear we would expect the best fit for K to be small so it is more flexible
